{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most of the applications of Machine Learning today are based on supervised learning (and as a result, this is where most of the investments go to), the vast majority of the available data is unlabeled: we have the input features X, but we do not have the labels y. The computer scientist Yann LeCun famously said that “if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.” In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you want to create a system that will take a few pictures of each item on a manufacturing production line and detect which items are defective. You can fairly easily create a system that will take pictures automatically, and this might give you thousands of pictures every day. You can then build a reasonably large dataset in just a few weeks. But wait, there are no labels! If you want to train a regular binary classifier that will predict whether an item is defective or not, you will need to label every single picture as “defective” or “normal.” This will generally require human experts to sit down and manually go through all the pictures. This is a long, costly, and tedious task, so it will usually only be done on a small subset of the available pictures. As a result, the labeled dataset will be quite small, and the classifier’s performance will be disappointing. Moreover, every time the company makes any change to its products, the whole process will need to be started over from scratch. Wouldn’t it be great if the algorithm could just exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering: The goal is to group similar instances together into clusters. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n",
    "- Anomaly detection: The objective is to learn what “normal” data looks like, and then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series\n",
    "- Density estimation: This is the task of estimating the probability density function (PDF) of the random process that generated the dataset. Density estimation is commonly used for anomaly detection: instances located in very low density regions are likely to be anomalies. It is also useful for data analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look around and you notice a few more. They are not identical, yet they are sufficiently similar for you to know that they most likely belong to the same species (or at least the same genus). You may need a botanist to tell you what species that is, but you certainly don’t need an expert to identify groups of similar-looking objects. This is called clustering: it is the task of identifying similar instances and assigning them to clusters, or groups of similar instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in classification, each instance gets assigned to a group. However, unlike classification, clustering is an unsupervised task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering use cases:**\n",
    "- For customer segmentation: You can cluster your customers based on their purchases and their activity on your website. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, customer segmentation can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.\n",
    "- For data analysis: When you analyze a new dataset, it can be helpful to run a clustering algorithm, and then analyze each cluster separately.\n",
    "- As a dimensionality reduction technique: Once a dataset has been clustered, it is usually possible to measure each instance’s affinity with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k-dimensional. This vector is typically much lower-dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
    "- For anomaly detection (outlier detection): Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second. Anomaly detection is particularly useful in detecting defects in manufacturing, or for fraud detection.\n",
    "- For semi-supervised learning: If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance\n",
    "- For search engines: Some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is use the trained clustering model to find this image’s cluster, and you can then simply return all the images from this cluster.\n",
    "- To segment an image: By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in the image. Image segmentation is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1 K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was proposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code modulation, but it was only published outside of the company in 1982. In 1965, Edward W. Forgy had published virtually the same algorithm, so K-Means is sometimes referred to as Lloyd–Forgy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clustering, an instance’s label is the index of the cluster that this instance gets assigned to by the algorithm: this is not to be confused with the class labels in classification (remember that clustering is an unsupervised learning task). The KMeans instance preserves a copy of the labels of the instances it was trained on, available via the labels_ instance variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 3, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 3, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46679593,  2.28585348],\n",
       "       [-2.80389616,  1.80117999],\n",
       "       [ 0.20876306,  2.25551336],\n",
       "       [-2.79290307,  2.79641063],\n",
       "       [-2.80037642,  1.30082566]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an instance to |a cluster is the distance to the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of assigning each instance to a single cluster, which is called hard clustering, it can be useful to give each instance a score per cluster, which is called soft clustering. The score can be the distance between the instance and the centroid; conversely, it can be a similarity score (or affinity), such as the Gaussian Radial Basis Function. In the KMeans class, the transform() method measures the distance from each instance to every centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.49439034, 2.81093633, 0.32995317, 2.9042344 , 2.88633901],\n",
       "       [4.4759332 , 5.80730058, 2.80290755, 5.84739223, 5.84236351],\n",
       "       [1.69136631, 1.21475352, 3.29399768, 0.29040966, 1.71086031],\n",
       "       [1.54808703, 0.72581411, 3.21806371, 0.36159148, 1.21567622]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the first instance in X_new is located at a distance of 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid. If you have a high-dimensional dataset and you transform it this way, you end up with a k-dimensional dataset: this transformation can be a very efficient nonlinear dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how does the algorithm work? Well, suppose you were given the centroids. You could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you could easily locate all the centroids by computing the mean of the instances for each cluster. But you are given neither the labels nor the centroids, so how can you proceed? Well, just start by placing the centroids randomly (e.g., by picking k instances at random and using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, and so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps (usually quite small); it will not oscillate forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational complexity of the algorithm is generally linear with regard to the number of instances m, the number of clusters k, and the number of dimensions n. However, this is only true when the data has a clustering structure. If it does not, then in the worst-case scenario the complexity can increase exponentially with the number of instances. In practice, this rarely happens, and K-Means is generally one of the fastest clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): whether it does or not depends on the centroid initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid initialization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the init hyperparameter to a NumPy array containing the list of centroids, and set n_init to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters = 5, init = good_init, n_init = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to run the algorithm multiple times with different random initializations and keep the best solution. The number of random initializations is controlled by the n_init hyperparameter: by default, it is equal to 10, which means that the whole algorithm described earlier runs 10 times when you call fit(), and Scikit-Learn keeps the best solution. But how exactly does it know which solution is the best? It uses a performance metric! That metric is called the model’s inertia, which is the mean squared distance between each instance and its closest centroid. The KMeans class runs the algorithm n_init times and keeps the model with the lowest inertia. If you are curious, a model’s inertia is accessible via the inertia_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211.62337889822362"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-211.6233788982236"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score() method returns the negative inertia. Why negative? Because a predictor’s score() method must always respect Scikit-Learn’s “greater is better” rule: if a predictor is better than another, its score() method should return a greater score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important improvement to the K-Means algorithm, K-Means++, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii. They introduced a smarter initialization step that tends to select centroids that are distant from one another, and this improvement makes the K-Means algorithm much less likely to converge to a suboptimal solution. They showed that the additional computation required for the smarter initialization step is well worth it because it makes it possible to drastically reduce the number of times the algorithm needs to be run to find the optimal solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans class uses this initialization method by default. If you want to force it to use the original method (i.e., picking k instances randomly to define the initial centroids), then you can set the init hyperparameter to \"random\". You will rarely need to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated K-means and mini-batch K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan. It considerably accelerates the algorithm by avoiding many unnecessary distance calculations. Elkan achieved this by exploiting the triangle inequality (i.e., that a straight line is always the shortest distance between two points ) and by keeping track of lower and upper bounds for distances between instances and centroids. This is the algorithm the KMeans class uses by default (you can force it to use the original algorithm by setting the algorithm hyperparameter to \"full\", although you probably will never need to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley. Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of three or four and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. You can just use this class like the KMeans class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(n_clusters=5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters = 5)\n",
    "minibatch_kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset does not fit in memory, the simplest option is to use the memmap class, as we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch at a time to the partial_fit() method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Mini-batch K-Means algorithm is much faster than the regularK-Means algorithm, its inertia is generally slightly worse, especially as the number of clusters increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that we could just pick the model with the lowest inertia, right? Unfortunately, it is not that simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inertia is not a good performance metric when trying to choose k because it keeps getting lower as we increase k. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the inertia as a function of the number of clusters k, the curve often contains an inflexion point called the “elbow”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique for choosing the best value for the number of clusters is rather coarse. A more precise approach (but also more computationally expensive) is to use the silhouette score, which is the mean silhouette coefficient over all the instances. An instance’s silhouette coefficient is equal to (b – a) / max(a, b), where a is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and b is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to –1 means that the instance may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the silhouette score, you can use Scikit-Learn’s silhouette_score() function, giving it all the instances in the dataset and the labels they were assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655517642572828"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more informative visualization is obtained when you plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a silhouette diagram. Each diagram contains one knife shape per cluster. The shape’s height indicates the number of instances the cluster contains, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better). The dashed line indicates the mean silhouette coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.2 Limits of K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to scale the input features before you run K-Means, or the clusters may be very stretched and K-Means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.3 Using clustering for image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation is the task of partitioning an image into multiple segments. In semantic segmentation, all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the “pedestrian” segment (there would be one segment containing all the pedestrians). In instance segmentation, all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian. The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to do something much simpler: color segmentation. We will simply assign pixels to the same segment if they have a similar color. In some applications, this may be sufficient. For example, if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread \n",
    "\n",
    "image = imread(os.path.join('images', 'unsupervised_learning', 'ladybug.png'))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is represented as a 3D array. The first dimension’s size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green, and blue, each between 0.0 and 1.0 (or between 0 and 255, if you use imageio.imread()). Some images may have fewer channels, such as\n",
    "grayscale images (one channel). And some images may have more channels, such as images with an additional alpha channel for transparency or satellite images, which often contain channels for many light frequencies (e.g., infrared)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image.reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters = 8).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, it may identify a color cluster for all shades of green. Next, for each color (e.g., dark green), it looks for the mean color of the pixel’s color cluster. For example, all shades of green may be replaced with the same light green color (assuming the mean color of the green cluster is light green). Finally, it reshapes this long list of colors to get the same shape as the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.4 Using clustering for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "log_reg = LogisticRegression(max_iter = 10000)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('kmeans', KMeans(n_clusters=50)),\n",
       "                ('log_reg', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('kmeans', KMeans(n_clusters = 50)),\n",
    "    ('log_reg', LogisticRegression(max_iter = 10000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we chose the number of clusters k arbitrarily; we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for k is much simpler than earlier. There’s no need to perform silhouette analysis or minimize the inertia; the best value of k is simply the one that results in the best classification performance during cross-validation. We can use GridSearchCV to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 98 candidates, totalling 294 fits\n",
      "[CV] kmeans__n_clusters=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=2, total=   0.5s\n",
      "[CV] kmeans__n_clusters=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=2, total=   0.4s\n",
      "[CV] kmeans__n_clusters=2 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=2, total=   0.4s\n",
      "[CV] kmeans__n_clusters=3 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=3, total=   1.3s\n",
      "[CV] kmeans__n_clusters=3 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=3, total=   1.6s\n",
      "[CV] kmeans__n_clusters=3 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=3, total=   1.1s\n",
      "[CV] kmeans__n_clusters=4 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=4, total=   2.2s\n",
      "[CV] kmeans__n_clusters=4 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=4, total=   2.8s\n",
      "[CV] kmeans__n_clusters=4 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=4, total=   1.1s\n",
      "[CV] kmeans__n_clusters=5 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=5, total=   3.7s\n",
      "[CV] kmeans__n_clusters=5 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=5, total=   2.5s\n",
      "[CV] kmeans__n_clusters=5 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=5, total=   2.9s\n",
      "[CV] kmeans__n_clusters=6 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=6, total=   5.2s\n",
      "[CV] kmeans__n_clusters=6 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=6, total=   4.6s\n",
      "[CV] kmeans__n_clusters=6 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=6, total=   4.1s\n",
      "[CV] kmeans__n_clusters=7 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=7, total=   3.3s\n",
      "[CV] kmeans__n_clusters=7 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=7, total=   3.4s\n",
      "[CV] kmeans__n_clusters=7 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=7, total=   3.8s\n",
      "[CV] kmeans__n_clusters=8 ............................................\n",
      "[CV] ............................. kmeans__n_clusters=8, total=   5.0s\n",
      "[CV] kmeans__n_clusters=8 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=8, total=   5.4s\n",
      "[CV] kmeans__n_clusters=8 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=8, total=   5.5s\n",
      "[CV] kmeans__n_clusters=9 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=9, total=   5.3s\n",
      "[CV] kmeans__n_clusters=9 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=9, total=   5.7s\n",
      "[CV] kmeans__n_clusters=9 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................. kmeans__n_clusters=9, total=   6.6s\n",
      "[CV] kmeans__n_clusters=10 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=10, total=   5.7s\n",
      "[CV] kmeans__n_clusters=10 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=10, total=   8.0s\n",
      "[CV] kmeans__n_clusters=10 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=10, total=   6.3s\n",
      "[CV] kmeans__n_clusters=11 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=11, total=   5.1s\n",
      "[CV] kmeans__n_clusters=11 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=11, total=   8.1s\n",
      "[CV] kmeans__n_clusters=11 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=11, total=   8.0s\n",
      "[CV] kmeans__n_clusters=12 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=12, total=   5.4s\n",
      "[CV] kmeans__n_clusters=12 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=12, total=   5.7s\n",
      "[CV] kmeans__n_clusters=12 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=12, total=   5.5s\n",
      "[CV] kmeans__n_clusters=13 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=13, total=   5.6s\n",
      "[CV] kmeans__n_clusters=13 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=13, total=   4.4s\n",
      "[CV] kmeans__n_clusters=13 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=13, total=   5.6s\n",
      "[CV] kmeans__n_clusters=14 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=14, total=   3.6s\n",
      "[CV] kmeans__n_clusters=14 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=14, total=   5.6s\n",
      "[CV] kmeans__n_clusters=14 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=14, total=   5.4s\n",
      "[CV] kmeans__n_clusters=15 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=15, total=   4.2s\n",
      "[CV] kmeans__n_clusters=15 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=15, total=   4.0s\n",
      "[CV] kmeans__n_clusters=15 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=15, total=   5.4s\n",
      "[CV] kmeans__n_clusters=16 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=16, total=   6.0s\n",
      "[CV] kmeans__n_clusters=16 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=16, total=   5.0s\n",
      "[CV] kmeans__n_clusters=16 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=16, total=   5.4s\n",
      "[CV] kmeans__n_clusters=17 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=17, total=   5.5s\n",
      "[CV] kmeans__n_clusters=17 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=17, total=   5.3s\n",
      "[CV] kmeans__n_clusters=17 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=17, total=   3.7s\n",
      "[CV] kmeans__n_clusters=18 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=18, total=   5.3s\n",
      "[CV] kmeans__n_clusters=18 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=18, total=   5.3s\n",
      "[CV] kmeans__n_clusters=18 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=18, total=   4.9s\n",
      "[CV] kmeans__n_clusters=19 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=19, total=   3.4s\n",
      "[CV] kmeans__n_clusters=19 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=19, total=   5.3s\n",
      "[CV] kmeans__n_clusters=19 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=19, total=   4.8s\n",
      "[CV] kmeans__n_clusters=20 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=20, total=   5.2s\n",
      "[CV] kmeans__n_clusters=20 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=20, total=   6.3s\n",
      "[CV] kmeans__n_clusters=20 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=20, total=   5.9s\n",
      "[CV] kmeans__n_clusters=21 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=21, total=   4.3s\n",
      "[CV] kmeans__n_clusters=21 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=21, total=   5.8s\n",
      "[CV] kmeans__n_clusters=21 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=21, total=   4.4s\n",
      "[CV] kmeans__n_clusters=22 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=22, total=   5.4s\n",
      "[CV] kmeans__n_clusters=22 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=22, total=   5.1s\n",
      "[CV] kmeans__n_clusters=22 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=22, total=   5.7s\n",
      "[CV] kmeans__n_clusters=23 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=23, total=   4.0s\n",
      "[CV] kmeans__n_clusters=23 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=23, total=   5.5s\n",
      "[CV] kmeans__n_clusters=23 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=23, total=   4.5s\n",
      "[CV] kmeans__n_clusters=24 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=24, total=   5.5s\n",
      "[CV] kmeans__n_clusters=24 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=24, total=   5.4s\n",
      "[CV] kmeans__n_clusters=24 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=24, total=   5.7s\n",
      "[CV] kmeans__n_clusters=25 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=25, total=   4.1s\n",
      "[CV] kmeans__n_clusters=25 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=25, total=   6.0s\n",
      "[CV] kmeans__n_clusters=25 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=25, total=   5.7s\n",
      "[CV] kmeans__n_clusters=26 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=26, total=   5.5s\n",
      "[CV] kmeans__n_clusters=26 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=26, total=   5.4s\n",
      "[CV] kmeans__n_clusters=26 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=26, total=   4.1s\n",
      "[CV] kmeans__n_clusters=27 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=27, total=   4.4s\n",
      "[CV] kmeans__n_clusters=27 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=27, total=   4.6s\n",
      "[CV] kmeans__n_clusters=27 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=27, total=   5.9s\n",
      "[CV] kmeans__n_clusters=28 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=28, total=   5.8s\n",
      "[CV] kmeans__n_clusters=28 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=28, total=   4.2s\n",
      "[CV] kmeans__n_clusters=28 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=28, total=   5.6s\n",
      "[CV] kmeans__n_clusters=29 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=29, total=   5.4s\n",
      "[CV] kmeans__n_clusters=29 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=29, total=   5.4s\n",
      "[CV] kmeans__n_clusters=29 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=29, total=   4.1s\n",
      "[CV] kmeans__n_clusters=30 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=30, total=   2.9s\n",
      "[CV] kmeans__n_clusters=30 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=30, total=   2.6s\n",
      "[CV] kmeans__n_clusters=30 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=30, total=   4.3s\n",
      "[CV] kmeans__n_clusters=31 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=31, total=   5.4s\n",
      "[CV] kmeans__n_clusters=31 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=31, total=   5.4s\n",
      "[CV] kmeans__n_clusters=31 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=31, total=   5.6s\n",
      "[CV] kmeans__n_clusters=32 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=32, total=   5.4s\n",
      "[CV] kmeans__n_clusters=32 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=32, total=   4.0s\n",
      "[CV] kmeans__n_clusters=32 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=32, total=   6.0s\n",
      "[CV] kmeans__n_clusters=33 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=33, total=   4.5s\n",
      "[CV] kmeans__n_clusters=33 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=33, total=   4.9s\n",
      "[CV] kmeans__n_clusters=33 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=33, total=   3.4s\n",
      "[CV] kmeans__n_clusters=34 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=34, total=   6.0s\n",
      "[CV] kmeans__n_clusters=34 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=34, total=   3.5s\n",
      "[CV] kmeans__n_clusters=34 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=34, total=   4.7s\n",
      "[CV] kmeans__n_clusters=35 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=35, total=   6.2s\n",
      "[CV] kmeans__n_clusters=35 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=35, total=   2.8s\n",
      "[CV] kmeans__n_clusters=35 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=35, total=   5.6s\n",
      "[CV] kmeans__n_clusters=36 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=36, total=   4.7s\n",
      "[CV] kmeans__n_clusters=36 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=36, total=   3.9s\n",
      "[CV] kmeans__n_clusters=36 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=36, total=   4.3s\n",
      "[CV] kmeans__n_clusters=37 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=37, total=   5.3s\n",
      "[CV] kmeans__n_clusters=37 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=37, total=   4.1s\n",
      "[CV] kmeans__n_clusters=37 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=37, total=   4.8s\n",
      "[CV] kmeans__n_clusters=38 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=38, total=   5.2s\n",
      "[CV] kmeans__n_clusters=38 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=38, total=   4.5s\n",
      "[CV] kmeans__n_clusters=38 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=38, total=   7.0s\n",
      "[CV] kmeans__n_clusters=39 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=39, total=   9.3s\n",
      "[CV] kmeans__n_clusters=39 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=39, total=   2.9s\n",
      "[CV] kmeans__n_clusters=39 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=39, total=   6.1s\n",
      "[CV] kmeans__n_clusters=40 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=40, total=   4.8s\n",
      "[CV] kmeans__n_clusters=40 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=40, total=  16.5s\n",
      "[CV] kmeans__n_clusters=40 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=40, total=   4.2s\n",
      "[CV] kmeans__n_clusters=41 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=41, total=   4.7s\n",
      "[CV] kmeans__n_clusters=41 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=41, total=   3.1s\n",
      "[CV] kmeans__n_clusters=41 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=41, total=   3.3s\n",
      "[CV] kmeans__n_clusters=42 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=42, total=   5.6s\n",
      "[CV] kmeans__n_clusters=42 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=42, total=   3.2s\n",
      "[CV] kmeans__n_clusters=42 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=42, total=   4.5s\n",
      "[CV] kmeans__n_clusters=43 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=43, total=   5.1s\n",
      "[CV] kmeans__n_clusters=43 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=43, total=   5.2s\n",
      "[CV] kmeans__n_clusters=43 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=43, total=   3.9s\n",
      "[CV] kmeans__n_clusters=44 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=44, total=   7.3s\n",
      "[CV] kmeans__n_clusters=44 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=44, total=   4.6s\n",
      "[CV] kmeans__n_clusters=44 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=44, total=   6.4s\n",
      "[CV] kmeans__n_clusters=45 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=45, total=   6.8s\n",
      "[CV] kmeans__n_clusters=45 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=45, total=   3.5s\n",
      "[CV] kmeans__n_clusters=45 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=45, total=   6.0s\n",
      "[CV] kmeans__n_clusters=46 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=46, total=   7.3s\n",
      "[CV] kmeans__n_clusters=46 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=46, total=   7.1s\n",
      "[CV] kmeans__n_clusters=46 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=46, total=   5.5s\n",
      "[CV] kmeans__n_clusters=47 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=47, total=   6.8s\n",
      "[CV] kmeans__n_clusters=47 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=47, total=   2.6s\n",
      "[CV] kmeans__n_clusters=47 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=47, total=   6.0s\n",
      "[CV] kmeans__n_clusters=48 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=48, total=   7.6s\n",
      "[CV] kmeans__n_clusters=48 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=48, total=   5.8s\n",
      "[CV] kmeans__n_clusters=48 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=48, total=   4.0s\n",
      "[CV] kmeans__n_clusters=49 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=49, total=   3.5s\n",
      "[CV] kmeans__n_clusters=49 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=49, total=   3.4s\n",
      "[CV] kmeans__n_clusters=49 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=49, total=   4.1s\n",
      "[CV] kmeans__n_clusters=50 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=50, total=   6.2s\n",
      "[CV] kmeans__n_clusters=50 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=50, total=   4.6s\n",
      "[CV] kmeans__n_clusters=50 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=50, total=   6.2s\n",
      "[CV] kmeans__n_clusters=51 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=51, total=   5.4s\n",
      "[CV] kmeans__n_clusters=51 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=51, total=   2.4s\n",
      "[CV] kmeans__n_clusters=51 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=51, total=   5.1s\n",
      "[CV] kmeans__n_clusters=52 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=52, total=   5.5s\n",
      "[CV] kmeans__n_clusters=52 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=52, total=   4.4s\n",
      "[CV] kmeans__n_clusters=52 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=52, total=   2.4s\n",
      "[CV] kmeans__n_clusters=53 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=53, total=   5.2s\n",
      "[CV] kmeans__n_clusters=53 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=53, total=   3.0s\n",
      "[CV] kmeans__n_clusters=53 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=53, total=   3.7s\n",
      "[CV] kmeans__n_clusters=54 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=54, total=   6.6s\n",
      "[CV] kmeans__n_clusters=54 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=54, total=   4.3s\n",
      "[CV] kmeans__n_clusters=54 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=54, total=   5.4s\n",
      "[CV] kmeans__n_clusters=55 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=55, total=   4.4s\n",
      "[CV] kmeans__n_clusters=55 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=55, total=   2.3s\n",
      "[CV] kmeans__n_clusters=55 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=55, total=   4.5s\n",
      "[CV] kmeans__n_clusters=56 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=56, total=   3.9s\n",
      "[CV] kmeans__n_clusters=56 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=56, total=   3.7s\n",
      "[CV] kmeans__n_clusters=56 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=56, total=   4.5s\n",
      "[CV] kmeans__n_clusters=57 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=57, total=   3.8s\n",
      "[CV] kmeans__n_clusters=57 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=57, total=   3.6s\n",
      "[CV] kmeans__n_clusters=57 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=57, total=   4.2s\n",
      "[CV] kmeans__n_clusters=58 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=58, total=   4.0s\n",
      "[CV] kmeans__n_clusters=58 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=58, total=   2.0s\n",
      "[CV] kmeans__n_clusters=58 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=58, total=   3.6s\n",
      "[CV] kmeans__n_clusters=59 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=59, total=   6.3s\n",
      "[CV] kmeans__n_clusters=59 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=59, total=   3.8s\n",
      "[CV] kmeans__n_clusters=59 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=59, total=   3.1s\n",
      "[CV] kmeans__n_clusters=60 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=60, total=   3.2s\n",
      "[CV] kmeans__n_clusters=60 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=60, total=   2.9s\n",
      "[CV] kmeans__n_clusters=60 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=60, total=   4.2s\n",
      "[CV] kmeans__n_clusters=61 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=61, total=   4.5s\n",
      "[CV] kmeans__n_clusters=61 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=61, total=   2.5s\n",
      "[CV] kmeans__n_clusters=61 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=61, total=   5.6s\n",
      "[CV] kmeans__n_clusters=62 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=62, total=   3.9s\n",
      "[CV] kmeans__n_clusters=62 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=62, total=   6.3s\n",
      "[CV] kmeans__n_clusters=62 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=62, total=   6.4s\n",
      "[CV] kmeans__n_clusters=63 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=63, total=   5.0s\n",
      "[CV] kmeans__n_clusters=63 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=63, total=   2.4s\n",
      "[CV] kmeans__n_clusters=63 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=63, total=   2.6s\n",
      "[CV] kmeans__n_clusters=64 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=64, total=   3.8s\n",
      "[CV] kmeans__n_clusters=64 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=64, total=   3.5s\n",
      "[CV] kmeans__n_clusters=64 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=64, total=   3.9s\n",
      "[CV] kmeans__n_clusters=65 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=65, total=   3.3s\n",
      "[CV] kmeans__n_clusters=65 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=65, total=   4.5s\n",
      "[CV] kmeans__n_clusters=65 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=65, total=   5.6s\n",
      "[CV] kmeans__n_clusters=66 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=66, total=   3.3s\n",
      "[CV] kmeans__n_clusters=66 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=66, total=   4.1s\n",
      "[CV] kmeans__n_clusters=66 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=66, total=   3.3s\n",
      "[CV] kmeans__n_clusters=67 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=67, total=   3.7s\n",
      "[CV] kmeans__n_clusters=67 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=67, total=   2.2s\n",
      "[CV] kmeans__n_clusters=67 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=67, total=   3.5s\n",
      "[CV] kmeans__n_clusters=68 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=68, total=   3.5s\n",
      "[CV] kmeans__n_clusters=68 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=68, total=   2.1s\n",
      "[CV] kmeans__n_clusters=68 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=68, total=   4.4s\n",
      "[CV] kmeans__n_clusters=69 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=69, total=   4.5s\n",
      "[CV] kmeans__n_clusters=69 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=69, total=   2.1s\n",
      "[CV] kmeans__n_clusters=69 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=69, total=   4.9s\n",
      "[CV] kmeans__n_clusters=70 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=70, total=   6.5s\n",
      "[CV] kmeans__n_clusters=70 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=70, total=   2.3s\n",
      "[CV] kmeans__n_clusters=70 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=70, total=   7.0s\n",
      "[CV] kmeans__n_clusters=71 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=71, total=   6.0s\n",
      "[CV] kmeans__n_clusters=71 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=71, total=   3.0s\n",
      "[CV] kmeans__n_clusters=71 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=71, total=   3.1s\n",
      "[CV] kmeans__n_clusters=72 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=72, total=   8.6s\n",
      "[CV] kmeans__n_clusters=72 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=72, total=   2.9s\n",
      "[CV] kmeans__n_clusters=72 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=72, total=   2.5s\n",
      "[CV] kmeans__n_clusters=73 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=73, total=   7.1s\n",
      "[CV] kmeans__n_clusters=73 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=73, total=   4.5s\n",
      "[CV] kmeans__n_clusters=73 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=73, total=   2.4s\n",
      "[CV] kmeans__n_clusters=74 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=74, total=   4.3s\n",
      "[CV] kmeans__n_clusters=74 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=74, total=   4.2s\n",
      "[CV] kmeans__n_clusters=74 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=74, total=   4.6s\n",
      "[CV] kmeans__n_clusters=75 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=75, total=   2.8s\n",
      "[CV] kmeans__n_clusters=75 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=75, total=   2.9s\n",
      "[CV] kmeans__n_clusters=75 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=75, total=   6.2s\n",
      "[CV] kmeans__n_clusters=76 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=76, total=   6.6s\n",
      "[CV] kmeans__n_clusters=76 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=76, total=   2.2s\n",
      "[CV] kmeans__n_clusters=76 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=76, total=   4.2s\n",
      "[CV] kmeans__n_clusters=77 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=77, total=   2.4s\n",
      "[CV] kmeans__n_clusters=77 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=77, total=   2.2s\n",
      "[CV] kmeans__n_clusters=77 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=77, total=   4.2s\n",
      "[CV] kmeans__n_clusters=78 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=78, total=   5.2s\n",
      "[CV] kmeans__n_clusters=78 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=78, total=   4.5s\n",
      "[CV] kmeans__n_clusters=78 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=78, total=   7.0s\n",
      "[CV] kmeans__n_clusters=79 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=79, total=   2.4s\n",
      "[CV] kmeans__n_clusters=79 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=79, total=   2.3s\n",
      "[CV] kmeans__n_clusters=79 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=79, total=   3.8s\n",
      "[CV] kmeans__n_clusters=80 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=80, total=   6.3s\n",
      "[CV] kmeans__n_clusters=80 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=80, total=   2.5s\n",
      "[CV] kmeans__n_clusters=80 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=80, total=   5.8s\n",
      "[CV] kmeans__n_clusters=81 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=81, total=   3.9s\n",
      "[CV] kmeans__n_clusters=81 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=81, total=   3.8s\n",
      "[CV] kmeans__n_clusters=81 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=81, total=   4.9s\n",
      "[CV] kmeans__n_clusters=82 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=82, total=   5.3s\n",
      "[CV] kmeans__n_clusters=82 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=82, total=   2.6s\n",
      "[CV] kmeans__n_clusters=82 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=82, total=   2.2s\n",
      "[CV] kmeans__n_clusters=83 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=83, total=   5.3s\n",
      "[CV] kmeans__n_clusters=83 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=83, total=   2.3s\n",
      "[CV] kmeans__n_clusters=83 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=83, total=   4.4s\n",
      "[CV] kmeans__n_clusters=84 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=84, total=   2.3s\n",
      "[CV] kmeans__n_clusters=84 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=84, total=   2.5s\n",
      "[CV] kmeans__n_clusters=84 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=84, total=   2.7s\n",
      "[CV] kmeans__n_clusters=85 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=85, total=   6.7s\n",
      "[CV] kmeans__n_clusters=85 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=85, total=   4.3s\n",
      "[CV] kmeans__n_clusters=85 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=85, total=   2.5s\n",
      "[CV] kmeans__n_clusters=86 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=86, total=   6.8s\n",
      "[CV] kmeans__n_clusters=86 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=86, total=   2.4s\n",
      "[CV] kmeans__n_clusters=86 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=86, total=   3.2s\n",
      "[CV] kmeans__n_clusters=87 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=87, total=   3.7s\n",
      "[CV] kmeans__n_clusters=87 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=87, total=   2.2s\n",
      "[CV] kmeans__n_clusters=87 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=87, total=   4.6s\n",
      "[CV] kmeans__n_clusters=88 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=88, total=   2.1s\n",
      "[CV] kmeans__n_clusters=88 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=88, total=   3.3s\n",
      "[CV] kmeans__n_clusters=88 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=88, total=   2.5s\n",
      "[CV] kmeans__n_clusters=89 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=89, total=   2.4s\n",
      "[CV] kmeans__n_clusters=89 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=89, total=   3.5s\n",
      "[CV] kmeans__n_clusters=89 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=89, total=   5.8s\n",
      "[CV] kmeans__n_clusters=90 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=90, total=   2.4s\n",
      "[CV] kmeans__n_clusters=90 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=90, total=   2.5s\n",
      "[CV] kmeans__n_clusters=90 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=90, total=   3.2s\n",
      "[CV] kmeans__n_clusters=91 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=91, total=   3.4s\n",
      "[CV] kmeans__n_clusters=91 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=91, total=   2.4s\n",
      "[CV] kmeans__n_clusters=91 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=91, total=   2.2s\n",
      "[CV] kmeans__n_clusters=92 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=92, total=   6.7s\n",
      "[CV] kmeans__n_clusters=92 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=92, total=   3.0s\n",
      "[CV] kmeans__n_clusters=92 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=92, total=   2.2s\n",
      "[CV] kmeans__n_clusters=93 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=93, total=   2.5s\n",
      "[CV] kmeans__n_clusters=93 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=93, total=   4.0s\n",
      "[CV] kmeans__n_clusters=93 ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ kmeans__n_clusters=93, total=   6.8s\n",
      "[CV] kmeans__n_clusters=94 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=94, total=   2.8s\n",
      "[CV] kmeans__n_clusters=94 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=94, total=   2.3s\n",
      "[CV] kmeans__n_clusters=94 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=94, total=   3.5s\n",
      "[CV] kmeans__n_clusters=95 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=95, total=   3.5s\n",
      "[CV] kmeans__n_clusters=95 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=95, total=   5.1s\n",
      "[CV] kmeans__n_clusters=95 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=95, total=   3.6s\n",
      "[CV] kmeans__n_clusters=96 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=96, total=   2.5s\n",
      "[CV] kmeans__n_clusters=96 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=96, total=   4.8s\n",
      "[CV] kmeans__n_clusters=96 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=96, total=   3.8s\n",
      "[CV] kmeans__n_clusters=97 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=97, total=   2.4s\n",
      "[CV] kmeans__n_clusters=97 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=97, total=   4.1s\n",
      "[CV] kmeans__n_clusters=97 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=97, total=   2.3s\n",
      "[CV] kmeans__n_clusters=98 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=98, total=   5.6s\n",
      "[CV] kmeans__n_clusters=98 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=98, total=   2.5s\n",
      "[CV] kmeans__n_clusters=98 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=98, total=   3.2s\n",
      "[CV] kmeans__n_clusters=99 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=99, total=   5.0s\n",
      "[CV] kmeans__n_clusters=99 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=99, total=   6.0s\n",
      "[CV] kmeans__n_clusters=99 ...........................................\n",
      "[CV] ............................ kmeans__n_clusters=99, total=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 294 out of 294 | elapsed: 21.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kmeans', KMeans(n_clusters=50)),\n",
       "                                       ('log_reg',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             param_grid={'kmeans__n_clusters': range(2, 100)}, verbose=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(kmeans__n_clusters = range(2, 100))\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv = 3, verbose = 2)\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kmeans__n_clusters': 97}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.5 Using clustering for semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labeled = 50\n",
    "log_reg = LogisticRegression(max_iter = 10000)\n",
    "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8288888888888889"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis = 0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = np.array([\n",
    "    4, 8, 0, 6, 8, 3, 7, 7, 9, 2,\n",
    "    5, 5, 8, 5, 2, 1, 2, 9, 6, 1,\n",
    "    1, 6, 9, 0, 8, 3, 0, 7, 4, 1,\n",
    "    6, 5, 2, 4, 1, 8, 6, 3, 9, 2,\n",
    "    4, 2, 9, 4, 7, 6, 2, 3, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14666666666666667"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter = 10000)\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? This is called label propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype = np.int32)\n",
    "\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12666666666666668"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter = 10000)\n",
    "log_reg.fit(X_train, y_train_propagated)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we propagated each representative instance’s label to all the instances in the same cluster, including the instances located close to the cluster boundaries, which are more likely to be mislabeled. Let’s see what happens if we only propagate the labels to the 20% of the instances that are closest to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 20\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12222222222222222"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter = 10000)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Active learning:** \n",
    "\n",
    "To continue improving your model and your training set, the next step could be to do a few rounds of active learning, which is when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them. There are many different strategies for active learning, but one of the most common ones is called uncertainty sampling. Here is how it works:\n",
    "- The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances\n",
    "- The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) are given to the expert to be labeled\n",
    "- You iterate this process until the performance improvement stops being worth the labeling effort\n",
    "\n",
    "Other strategies include labeling the instances that would result in the largest model change, or the largest drop in the model’s validation error, or the instances that different models disagree on (e.g., an SVM or a Random Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.6 DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm defines clusters as continuous regions of high density. Here is how it works:\n",
    "- For each instance, the algorithm counts how many instances are located within a small distance ε (epsilon) from it. This region is called the instance’s ε-neighborhood.\n",
    "- If an instance has at least min_samples instances in its ε- neighborhood (including itself), then it is considered a core instance. In other words, core instances are those that are located in dense regions.\n",
    "- All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence of neighboring core instances forms a single cluster\n",
    "- Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm works well if all the clusters are dense enough and if they are well separated by low-density regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=0.05)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons \n",
    "\n",
    "X, y = make_moons(n_samples = 1000, noise = 0.05)\n",
    "dbscan = DBSCAN(eps = 0.05, min_samples = 5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 3, 6, 0, 1, 4, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some instances have a cluster index equal to –1, which means that they are considered as anomalies by the algorithm. The indices of the core instances are available in the core_sample_indices_ instance variable, and the core instances themselves are available in the components_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dbscan.core_sample_indices_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 12])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.core_sample_indices_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.66691505, -0.30652108],\n",
       "       [ 0.83408896,  0.47354944],\n",
       "       [-0.85653483,  0.46839591],\n",
       "       [ 0.70926809,  0.64707668],\n",
       "       [ 1.12408567, -0.48671387],\n",
       "       [ 1.52467405, -0.37180615],\n",
       "       [ 0.78418692,  0.66519322],\n",
       "       [ 2.03361811,  0.27280217],\n",
       "       [ 0.53420473,  0.80080493],\n",
       "       [ 1.00395247,  0.1220476 ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.components_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, the DBSCAN class does not have a predict() method, although it has a fit_predict() method. In other words, it cannot predict which cluster a new instance belongs to. This implementation decision was made because different classification algorithms can be better for different tasks, so the authors decided to let the user choose which one to use. Moreover, it’s not hard to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 5, 4])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "knn.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.92, 0.  , 0.  , 0.  , 0.08, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.96, 0.  , 0.  , 0.04],\n",
       "       [0.  , 0.  , 0.  , 0.24, 0.  , 0.74, 0.  , 0.02, 0.  , 0.  ],\n",
       "       [0.28, 0.  , 0.  , 0.  , 0.72, 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only trained the classifier on the core instances, but we could also have chosen to train it on all the instances, or all but the anomalies: this choice depends on the final task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that since there is no anomaly in the training set, the classifier always chooses a cluster, even when that cluster is far away. It is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, use the kneighbors() method of the KNeighborsClassifier. Given a set of instances, it returns the distances and the indices of the k nearest neighbors in the training set (two matrices, each with k columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  6,  5, -1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors = 1)\n",
    "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
    "y_pred[y_dist > 0.2] = -1\n",
    "y_pred.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. It is robust to outliers, and it has just two hyperparameters (eps and min_samples). If the density varies significantly across the clusters, however, it can be impossible for it to capture all the clusters properly. Its computational complexity is roughly O(m log m), making it pretty close to linear with regard to the number of instances, but Scikit-Learn’s implementation can require up to O(m ) memory if eps is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to try Hierarchical DBSCAN (HDBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.7 Other clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agglomerative clustering: A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other until there’s one big group of bubbles. Similarly, at each iteration, agglomerative clustering connects the nearest pair of clusters (starting with individual instances). If you drew a tree with a branch for every pair of clusters that merged, you would get a binary tree of clusters, where the leaves are the individual instances. This approach scales very well to large numbers of instances or clusters. It can capture clusters of various shapes, it produces a flexible and informative cluster tree instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix, which is a sparse m × m matrix that indicates which pairs of instances are neighbors (e.g., returned by sklearn.neighbors.kneighbors_graph()). Without a connectivity matrix, the algorithm does not scale well to large datasets.\n",
    "- BIRCH: The BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm was designed specifically for very large datasets, and it can be faster than batch K-Means, with similar results, as long as the number of features is not too large (<20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this approach allows it to use limited memory, while handling huge datasets.\n",
    "- Mean-Shift: This algorithm starts by placing a circle centered on each instance; then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting step until all the circles stop moving (i.e., until each of them is centered on the mean of the instances it contains). Mean-Shift shifts the circles in the direction of higher density, until each of them has found a local density maximum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean-Shift has some of the same features as DBSCAN, like how it can find any number of clusters of any shape, it has very few hyperparameters (just one—the radius of the circles, called the bandwidth), and it relies on local density estimation. But unlike DBSCAN, Mean-Shift tends to chop clusters into pieces when they have internal density variations. Unfortunately, its computational complexity is O(m^2), so it is not suited for large datasets.\n",
    "- Affinity propagation: This algorithm uses a voting system, where instances vote for similar instances to be their representatives, and once the algorithm converges, each representative and its voters form a cluster. Affinity propagation can detect any number of clusters of different sizes. Unfortunately, this algorithm has a computational complexity of O(m^2), so it too is not suited for large datasets\n",
    "- Spectral clustering: This algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces its dimensionality), then it uses another clustering algorithm in this lowdimensional space (Scikit-Learn’s implementation uses K-Means.) Spectral clustering can capture complex cluster structures, and it can also be used to cut graphs (e.g., to identify clusters of friends on a social network). It does not scale well to large numbers of instances, and it does not behave well when the clusters have very different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be used for density estimation, clustering, and anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and orientation. When you observe an instance, you know it was generated from one of the Gaussian distributions, but you are not told which one, and you do not know what the parameters of these distributions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several GMM variants. In the simplest variant, implemented in the GaussianMixture class, you must know in advance the number k of Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components = 3, n_init = 10)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.weights_\n",
    "gm.means_\n",
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.converged_\n",
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.predict(X)\n",
    "gm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian mixture model is a generative model, meaning you can sample new instances from it (note that they are ordered by cluster index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = gm.sample(6)\n",
    "X_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to estimate the density of the model at any given location. This is achieved using the score_samples() method: for each instance it is given, this method estimates the log of the probability density function (PDF) at that location. The greater the score, the higher the density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.score_samples(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the exponential of these scores, you get the value of the PDF at the location of the given instances. These are not probabilities, but probability densities: they can take on any positive value, not just a value between 0 and 1. To estimate the probability that an instance will fall within a particular region, you would have to integrate the PDF over that region (if you do so over the entire space of possible instance locations, the result will be 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational complexity of training a GaussianMixture model depends on the number of instances m, the number of dimensions n, the number of clusters k, and the constraints on the covariance matrices. If covariance_type is \"spherical or \"diag\", it is O(kmn), assuming the data has a clustering structure. If covariance_type is \"tied\" or \"full\", it is O(kmn^2+ kn^3), so it will not scale to large numbers of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.1 Anomaly detection using gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection (also called outlier detection) is the task of detecting instances that deviate strongly from the norm. These instances are called anomalies, or outliers, while the normal instances are called inliers. Anomaly detection is useful in a wide variety of applications, such as fraud detection, detecting defective products in manufacturing, or removing outliers from a dataset before training another model (which can significantly improve the performance of the resulting model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Gaussian mixture model for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known. Say it is equal to 4%. You then set the density threshold to be the value that results in having 4% of the instances located in areas below that threshold density. If you notice that you get too many false positives (i.e., perfectly good products that are flagged as defective), you can lower the threshold. Conversely, if you have too many false negatives (i.e., defective products that the system does not flag as defective), you can increase the threshold. This is the usual precision/recall trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closely related task is novelty detection: it differs from anomaly detection in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption. Indeed, outlier detection is often used to clean up a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models try to fit all the data, including the outliers, so if you have too many of them, this will bias the model’s view of “normality,” and some outliers may wrongly be considered as normal. If this happens, you can try to fit the model once, use it to detect and remove the most extreme outliers, then fit the model again on the cleaned-up dataset. Another approach is to use robust covariance estimation methods (see the EllipticEnvelope class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.2 Selecting the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K-Means, you could use the inertia or the silhouette score to select the appropriate number of clusters. But with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes. Instead, you can try to find the model that minimizes a theoretical information criterion, such as the Bayesian information criterion (BIC) or the Akaike information criterion (AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters) and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by the AIC, but tends to not fit the data quite as well (this is especially true for larger datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.bic(X)\n",
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.3 Bayesian gaussian mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than manually searching for the optimal number of clusters, you can use the BayesianGaussianMixture class, which is capable of giving weights equal (or close) to zero to unnecessary clusters. Set the number of clusters n_components to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components = 10, n_init = 10)\n",
    "bgm.fit(X)\n",
    "np.round(bgm.weights_, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.4 Other algorithms for anomaly and novelty detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA (and other dimensionality reduction techniques with an inverse_transform() method): If you compare the reconstruction error of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and often quite efficient anomaly detection approach\n",
    "- Fast-MCD (minimum covariance determinant): Implemented by the EllipticEnvelope class, this algorithm is useful for outlier detection, in particular to clean up a dataset. It assumes that the normal instances (inliers) are generated from a single Gaussian distribution (not a mixture). It also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution. When the algorithm estimates the parameters of the Gaussian distribution (i.e., the shape of the elliptic envelope around the inliers), it is careful to ignore the instances that are most likely outliers. This technique gives a better estimation of the elliptic envelope and thus makes the algorithm better at identifying the outliers.\n",
    "- Isolation forest: This is an efficient algorithm for outlier detection, especially in highdimensional datasets. The algorithm builds a Random Forest in which each Decision Tree is grown randomly: at each node, it picks a feature randomly, then it picks a random threshold value (between the min and max values) to split the dataset in two. The dataset gradually gets chopped into pieces this way, until all instances end up isolated from the other instances. Anomalies are usually far from other instances, so on average (across all the Decision Trees) they tend to get isolated in fewer steps than normal instances.\n",
    "- Local Outlier Factor (LOF): This algorithm is also good for outlier detection. It compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its k nearest neighbors\n",
    "- One-class SVM: This algorithm is better suited for novelty detection. Recall that a kernelized SVM classifier separates two classes by first (implicitly) mapping all the instances to a high-dimensional space, then separating the two classes using a linear SVM classifier within this highdimensional space (see Chapter 5). Since we just have one class of instances, the one-class SVM algorithm instead tries to separate the instances in high-dimensional space from the origin. In the original space, this will correspond to finding a small region that encompasses ll the instances. If a new instance does not fall within this region, it is n anomaly. There are a few hyperparameters to tweak: the usual ones or a kernelized SVM, plus a margin hyperparameter that corresponds to he probability of a new instance being mistakenly considered as novel hen it is in fact normal. It works great, especially with highdimensional atasets, but like all SVMs it does not scale to large atasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
